{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYsbPTcDDS9L"
      },
      "source": [
        "# How to Implement the Backpropagation Algorithm From Scratch in Python\n",
        "\n",
        "Based on the tutorial by Jason Brownlee — [Machine Learning Mastery](https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/)\n",
        "\n",
        "---\n",
        "\n",
        "**Backpropagation** is a supervised learning algorithm for training multilayer feed-forward neural networks. It works by propagating the error signal backward through the network and using **gradient descent** to update weights.\n",
        "\n",
        "In this notebook we will:\n",
        "- Initialize a neural network\n",
        "- Forward propagate inputs\n",
        "- Backpropagate errors\n",
        "- Train the network with stochastic gradient descent\n",
        "- Make predictions\n",
        "- Apply the algorithm to the Wheat Seeds dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1Rr313TDS9T"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUIyndffDS9U"
      },
      "outputs": [],
      "source": [
        "from math import exp\n",
        "from random import seed, random\n",
        "from csv import reader\n",
        "from random import randrange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0X92JJFDS9Z"
      },
      "source": [
        "---\n",
        "## 1. Initialize Network\n",
        "\n",
        "We represent a network as a **list of layers**, where each layer is a **list of neurons**. Each neuron is a dictionary containing its `weights` (one per input + one bias) and other training properties.\n",
        "\n",
        "The network architecture:\n",
        "- **Input layer**: the raw feature values (not stored explicitly)\n",
        "- **Hidden layer**: `n_hidden` neurons, each connected to all inputs\n",
        "- **Output layer**: one neuron per class\n",
        "\n",
        "Weights are initialized **randomly** in the range `[0, 1)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJKpnJexDS9a"
      },
      "outputs": [],
      "source": [
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "    \"\"\"\n",
        "    Create and initialize a new neural network.\n",
        "    Each neuron: {'weights': [w1, w2, ..., bias]}\n",
        "    \"\"\"\n",
        "    network = []\n",
        "    # Hidden layer: n_hidden neurons, each with (n_inputs + 1) weights (last = bias)\n",
        "    hidden_layer = [{'weights': [random() for _ in range(n_inputs + 1)]} for _ in range(n_hidden)]\n",
        "    network.append(hidden_layer)\n",
        "    # Output layer: n_outputs neurons, each with (n_hidden + 1) weights\n",
        "    output_layer = [{'weights': [random() for _ in range(n_hidden + 1)]} for _ in range(n_outputs)]\n",
        "    network.append(output_layer)\n",
        "    return network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh9n1Pe9DS9a"
      },
      "outputs": [],
      "source": [
        "# Example: 2 inputs, 1 hidden neuron, 2 outputs\n",
        "seed(1)\n",
        "network = initialize_network(2, 1, 2)\n",
        "for layer_idx, layer in enumerate(network):\n",
        "    print(f\"Layer {layer_idx + 1}:\")\n",
        "    for neuron in layer:\n",
        "        print(f\"  Neuron weights: {neuron['weights']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud8EpwZLDS9b"
      },
      "source": [
        "---\n",
        "## 2. Forward Propagation\n",
        "\n",
        "Forward propagation computes the **output of each neuron** layer by layer until we reach the final output.\n",
        "\n",
        "### 2.1 Neuron Activation\n",
        "\n",
        "The activation of a neuron is the **weighted sum** of inputs plus a bias:\n",
        "\n",
        "$$\\text{activation} = \\sum_{i=1}^{n} w_i \\cdot x_i + b$$\n",
        "\n",
        "where $w_i$ are the weights, $x_i$ are the inputs, and $b$ is the bias term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2QIVtvXDS9b"
      },
      "outputs": [],
      "source": [
        "def activate(weights, inputs):\n",
        "    \"\"\"\n",
        "    Calculate neuron activation: dot(weights, inputs) + bias\n",
        "    The last weight is the bias (paired with a constant input of 1).\n",
        "    \"\"\"\n",
        "    activation = weights[-1]  # bias\n",
        "    for i in range(len(weights) - 1):\n",
        "        activation += weights[i] * inputs[i]\n",
        "    return activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyrmeHFLDS9c"
      },
      "source": [
        "### 2.2 Sigmoid Transfer Function\n",
        "\n",
        "After computing the activation, we pass it through an **activation function** to introduce non-linearity. We use the **sigmoid function**:\n",
        "\n",
        "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "\n",
        "The sigmoid maps any real number to the range $(0, 1)$, making it ideal for classification problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiVWHKuSDS9c"
      },
      "outputs": [],
      "source": [
        "def transfer(activation):\n",
        "    \"\"\"\n",
        "    Sigmoid activation function: σ(x) = 1 / (1 + e^(-x))\n",
        "    Output is always in range (0, 1).\n",
        "    \"\"\"\n",
        "    return 1.0 / (1.0 + exp(-activation))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "craTmc6oDS9d"
      },
      "source": [
        "### 2.3 Forward Propagation Through Layers\n",
        "\n",
        "The output of each layer becomes the **input to the next layer**. For each neuron, we compute the activation and store the output as `'output'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy586xENDS9e"
      },
      "outputs": [],
      "source": [
        "def forward_propagate(network, row):\n",
        "    \"\"\"\n",
        "    Forward propagate one row through the network.\n",
        "    Returns the final output layer activations.\n",
        "    \"\"\"\n",
        "    inputs = row\n",
        "    for layer in network:\n",
        "        new_inputs = []\n",
        "        for neuron in layer:\n",
        "            activation = activate(neuron['weights'], inputs)\n",
        "            neuron['output'] = transfer(activation)\n",
        "            new_inputs.append(neuron['output'])\n",
        "        inputs = new_inputs  # outputs of this layer become inputs to next\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLzJdBtCDS9e"
      },
      "outputs": [],
      "source": [
        "# Test forward propagation\n",
        "seed(1)\n",
        "network = initialize_network(2, 1, 2)\n",
        "row = [1, 0, None]  # 2 inputs + class label placeholder\n",
        "output = forward_propagate(network, row)\n",
        "print(\"Network output:\", output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UbfCPczDS9f"
      },
      "source": [
        "---\n",
        "## 3. Backpropagation\n",
        "\n",
        "Backpropagation calculates the **error gradient** for each neuron and propagates it backward through the network to update the weights.\n",
        "\n",
        "### 3.1 Sigmoid Derivative\n",
        "\n",
        "To update weights using gradient descent, we need the **derivative of the sigmoid function**. Given that we already have the sigmoid output $\\sigma(x)$:\n",
        "\n",
        "$$\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$$\n",
        "\n",
        "This is efficient since we already stored the output from forward propagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzoOSIIjDS9f"
      },
      "outputs": [],
      "source": [
        "def transfer_derivative(output):\n",
        "    \"\"\"\n",
        "    Derivative of the sigmoid function.\n",
        "    σ'(x) = σ(x) * (1 - σ(x))\n",
        "    We use the already-computed output to avoid recalculation.\n",
        "    \"\"\"\n",
        "    return output * (1.0 - output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRKvze9nDS9f"
      },
      "source": [
        "### 3.2 Computing Error Signals (Delta)\n",
        "\n",
        "**Output layer error** — The delta for each output neuron is:\n",
        "\n",
        "$$\\delta^{\\text{out}}_j = (\\hat{y}_j - y_j) \\cdot \\sigma'(\\text{out}_j)$$\n",
        "\n",
        "where $\\hat{y}_j$ is the predicted output and $y_j$ is the expected output (one-hot encoded).\n",
        "\n",
        "**Hidden layer error** — The delta is the weighted sum of errors from the next layer:\n",
        "\n",
        "$$\\delta^{\\text{hidden}}_j = \\left(\\sum_k w_{jk} \\cdot \\delta^{\\text{out}}_k\\right) \\cdot \\sigma'(\\text{out}_j)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L7EO_qLDS9f"
      },
      "outputs": [],
      "source": [
        "def backward_propagate_error(network, expected):\n",
        "    \"\"\"\n",
        "    Backpropagate error and store delta in each neuron.\n",
        "    'expected' is the one-hot encoded expected output.\n",
        "    \"\"\"\n",
        "    for i in reversed(range(len(network))):\n",
        "        layer = network[i]\n",
        "        errors = []\n",
        "\n",
        "        if i != len(network) - 1:\n",
        "            # Hidden layers: weighted sum of errors from next layer\n",
        "            for j in range(len(layer)):\n",
        "                error = sum(neuron['weights'][j] * neuron['delta'] for neuron in network[i + 1])\n",
        "                errors.append(error)\n",
        "        else:\n",
        "            # Output layer: difference between prediction and expected\n",
        "            for j in range(len(layer)):\n",
        "                neuron = layer[j]\n",
        "                errors.append(neuron['output'] - expected[j])\n",
        "\n",
        "        # Compute delta = error * sigmoid_derivative(output)\n",
        "        for j in range(len(layer)):\n",
        "            neuron = layer[j]\n",
        "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQQU0gI5DS9g"
      },
      "source": [
        "### 3.3 Update Weights\n",
        "\n",
        "Once we have the delta for each neuron, we update the weights using **stochastic gradient descent**:\n",
        "\n",
        "$$w_i \\leftarrow w_i - \\eta \\cdot \\delta \\cdot x_i$$\n",
        "\n",
        "where $\\eta$ is the **learning rate** and $x_i$ is the input to the neuron (output of the previous layer). The bias weight is updated with $x_i = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG0s8u0EDS9g"
      },
      "outputs": [],
      "source": [
        "def update_weights(network, row, l_rate):\n",
        "    \"\"\"\n",
        "    Update weights using gradient descent.\n",
        "    w_i = w_i - learning_rate * delta * input_i\n",
        "    \"\"\"\n",
        "    for i in range(len(network)):\n",
        "        # Input to this layer: original row (for hidden) or previous layer outputs\n",
        "        inputs = row[:-1]  # exclude class label\n",
        "        if i != 0:\n",
        "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "        for neuron in network[i]:\n",
        "            for j in range(len(inputs)):\n",
        "                neuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
        "            neuron['weights'][-1] -= l_rate * neuron['delta']  # update bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7RqQl-gDS9h"
      },
      "source": [
        "---\n",
        "## 4. Train Network\n",
        "\n",
        "Training is done using **Stochastic Gradient Descent (SGD)**: for each epoch, we iterate over all training examples one at a time, forward propagate, backpropagate the error, and update weights.\n",
        "\n",
        "The **sum of squared errors (SSE)** is accumulated each epoch to monitor training progress:\n",
        "\n",
        "$$\\text{SSE} = \\sum_{j} (\\hat{y}_j - y_j)^2$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjLlhxfWDS9h"
      },
      "outputs": [],
      "source": [
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "    \"\"\"\n",
        "    Train a network using stochastic gradient descent.\n",
        "\n",
        "    Parameters:\n",
        "        network   : initialized network\n",
        "        train     : training dataset (last column = class label as integer)\n",
        "        l_rate    : learning rate η\n",
        "        n_epoch   : number of training epochs\n",
        "        n_outputs : number of output neurons (= number of classes)\n",
        "    \"\"\"\n",
        "    for epoch in range(n_epoch):\n",
        "        sum_error = 0\n",
        "        for row in train:\n",
        "            outputs = forward_propagate(network, row)\n",
        "            # One-hot encode the expected output\n",
        "            expected = [0] * n_outputs\n",
        "            expected[int(row[-1])] = 1\n",
        "            # Accumulate SSE\n",
        "            sum_error += sum((expected[j] - outputs[j])**2 for j in range(len(expected)))\n",
        "            backward_propagate_error(network, expected)\n",
        "            update_weights(network, row, l_rate)\n",
        "        print(f\"Epoch={epoch+1:4d}  learning_rate={l_rate:.3f}  error={sum_error:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kerxev7iDS9h"
      },
      "outputs": [],
      "source": [
        "# Test training on a small XOR-like dataset\n",
        "seed(1)\n",
        "dataset = [\n",
        "    [2.7810836,  2.550537003, 0],\n",
        "    [1.465489372, 2.362125076, 0],\n",
        "    [3.396561688, 4.400293529, 0],\n",
        "    [1.38807019,  1.850220317, 0],\n",
        "    [3.06407232,  3.005305973, 0],\n",
        "    [7.627531214, 2.759262235, 1],\n",
        "    [5.332441248, 2.088626775, 1],\n",
        "    [6.922596716, 1.77106367,  1],\n",
        "    [8.675418651, -0.242068655, 1],\n",
        "    [7.673756466, 3.508563011, 1]\n",
        "]\n",
        "\n",
        "n_inputs  = len(dataset[0]) - 1  # 2\n",
        "n_outputs = len(set(row[-1] for row in dataset))  # 2\n",
        "\n",
        "network = initialize_network(n_inputs, 2, n_outputs)\n",
        "train_network(network, dataset, l_rate=0.5, n_epoch=20, n_outputs=n_outputs)\n",
        "\n",
        "print(\"\\nFinal weights:\")\n",
        "for layer in network:\n",
        "    print(layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR76eNaUDS9h"
      },
      "source": [
        "---\n",
        "## 5. Predict\n",
        "\n",
        "After training, we can make predictions. We forward propagate a row and return the **index of the output neuron with the highest activation** (i.e., `argmax`):\n",
        "\n",
        "$$\\hat{y} = \\arg\\max_j \\; \\text{output}_j$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es4eHqEYDS9i"
      },
      "outputs": [],
      "source": [
        "def predict(network, row):\n",
        "    \"\"\"\n",
        "    Make a prediction for a given row.\n",
        "    Returns the index of the output neuron with the highest output (argmax).\n",
        "    \"\"\"\n",
        "    outputs = forward_propagate(network, row)\n",
        "    return outputs.index(max(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcG7NW_CDS9i"
      },
      "outputs": [],
      "source": [
        "# Test predictions on training data\n",
        "for row in dataset:\n",
        "    prediction = predict(network, row)\n",
        "    actual = int(row[-1])\n",
        "    status = \"✓\" if prediction == actual else \"✗\"\n",
        "    print(f\"Expected={actual}  Predicted={prediction}  {status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlZ5QRD-DS9i"
      },
      "source": [
        "---\n",
        "## 6. Seeds Dataset Case Study\n",
        "\n",
        "We now apply the full algorithm to a **real-world dataset** — the [Wheat Seeds Dataset](https://archive.ics.uci.edu/ml/datasets/seeds).\n",
        "\n",
        "- **7 input features**: geometric properties of wheat kernels (area, perimeter, compactness, etc.)\n",
        "- **3 output classes**: varieties of wheat (Kama=0, Rosa=1, Canadian=2)\n",
        "- **210 samples** total\n",
        "\n",
        "We evaluate using **5-fold cross-validation** with the following hyperparameters:\n",
        "- Learning rate: `0.3`\n",
        "- Epochs: `500`\n",
        "- Hidden neurons: `5`\n",
        "\n",
        "### 6.1 Helper Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVPerMl0DS9j"
      },
      "outputs": [],
      "source": [
        "def load_csv(filename):\n",
        "    \"\"\"Load a CSV file and return all rows as a list of lists.\"\"\"\n",
        "    dataset = []\n",
        "    with open(filename, 'r') as file:\n",
        "        csv_reader = reader(file)\n",
        "        for row in csv_reader:\n",
        "            if not row:\n",
        "                continue\n",
        "            dataset.append(row)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def str_column_to_float(dataset, column):\n",
        "    \"\"\"Convert a column of strings to floats.\"\"\"\n",
        "    for row in dataset:\n",
        "        row[column] = float(row[column].strip())\n",
        "\n",
        "\n",
        "def str_column_to_int(dataset, column):\n",
        "    \"\"\"Convert a string column to integer class labels (0-indexed).\"\"\"\n",
        "    class_values = sorted(set(row[column] for row in dataset))\n",
        "    lookup = {val: i for i, val in enumerate(class_values)}\n",
        "    for row in dataset:\n",
        "        row[column] = lookup[row[column]]\n",
        "    return lookup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49YRWk-aDS9j"
      },
      "source": [
        "### 6.2 Normalization\n",
        "\n",
        "We **normalize** each feature to the range $[0, 1]$ using **min-max scaling**:\n",
        "\n",
        "$$x'_i = \\frac{x_i - x_{\\min}}{x_{\\max} - x_{\\min}}$$\n",
        "\n",
        "This prevents features with large values from dominating the gradient updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsRCmWL1DS9k"
      },
      "outputs": [],
      "source": [
        "def dataset_minmax(dataset):\n",
        "    \"\"\"Find min and max values for each column.\"\"\"\n",
        "    minmax = []\n",
        "    for i in range(len(dataset[0])):\n",
        "        col_values = [row[i] for row in dataset]\n",
        "        minmax.append([min(col_values), max(col_values)])\n",
        "    return minmax\n",
        "\n",
        "\n",
        "def normalize_dataset(dataset, minmax):\n",
        "    \"\"\"Normalize each feature to range [0, 1] using min-max scaling.\"\"\"\n",
        "    for row in dataset:\n",
        "        for i in range(len(row) - 1):  # exclude label column\n",
        "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hwGzGCgDS9l"
      },
      "source": [
        "### 6.3 Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmxnfAjzDS9m"
      },
      "outputs": [],
      "source": [
        "def cross_validation_split(dataset, n_folds):\n",
        "    \"\"\"Split dataset into k folds for cross-validation.\"\"\"\n",
        "    dataset_split = []\n",
        "    dataset_copy = list(dataset)\n",
        "    fold_size = int(len(dataset) / n_folds)\n",
        "    for _ in range(n_folds):\n",
        "        fold = []\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "        dataset_split.append(fold)\n",
        "    return dataset_split\n",
        "\n",
        "\n",
        "def accuracy_metric(actual, predicted):\n",
        "    \"\"\"Calculate classification accuracy as a percentage.\"\"\"\n",
        "    correct = sum(1 for a, p in zip(actual, predicted) if a == p)\n",
        "    return correct / len(actual) * 100.0\n",
        "\n",
        "\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "    \"\"\"Evaluate algorithm using k-fold cross-validation.\"\"\"\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "    scores = []\n",
        "    for fold in folds:\n",
        "        train_set = [row for f in folds for row in f if f is not fold]\n",
        "        test_set  = [list(row) for row in fold]  # copy so labels can be removed\n",
        "        predicted = algorithm(train_set, test_set, *args)\n",
        "        actual    = [row[-1] for row in fold]\n",
        "        scores.append(accuracy_metric(actual, predicted))\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QApV7gTDS9n"
      },
      "source": [
        "### 6.4 Full Backpropagation Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unO3TkAfDS9p"
      },
      "outputs": [],
      "source": [
        "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
        "    \"\"\"\n",
        "    Full backpropagation algorithm:\n",
        "      1. Initialize network\n",
        "      2. Train with SGD\n",
        "      3. Predict on test set\n",
        "    \"\"\"\n",
        "    n_inputs  = len(train[0]) - 1\n",
        "    n_outputs = len(set(row[-1] for row in train))\n",
        "    network   = initialize_network(n_inputs, n_hidden, n_outputs)\n",
        "    train_network(network, train, l_rate, n_epoch, n_outputs)\n",
        "    predictions = [predict(network, row) for row in test]\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDzFXeP0DS9r"
      },
      "source": [
        "### 6.5 Download and Prepare the Seeds Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB32iCm0DS9r"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "\n",
        "# Download the dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv'\n",
        "urllib.request.urlretrieve(url, 'wheat-seeds.csv')\n",
        "print(\"Dataset downloaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1fhmJP9DS9r"
      },
      "source": [
        "### 6.6 Run Cross-Validation and Report Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N_en5IzDS9s"
      },
      "outputs": [],
      "source": [
        "seed(1)\n",
        "\n",
        "# Load and preprocess dataset\n",
        "dataset = load_csv('wheat-seeds.csv')\n",
        "for i in range(len(dataset[0]) - 1):\n",
        "    str_column_to_float(dataset, i)\n",
        "str_column_to_int(dataset, len(dataset[0]) - 1)  # convert class labels\n",
        "\n",
        "# Normalize features\n",
        "minmax = dataset_minmax(dataset)\n",
        "normalize_dataset(dataset, minmax)\n",
        "\n",
        "# Hyperparameters\n",
        "n_folds  = 5\n",
        "l_rate   = 0.3\n",
        "n_epoch  = 500\n",
        "n_hidden = 5\n",
        "\n",
        "# Evaluate\n",
        "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"Fold Scores: {[f'{s:.1f}%' for s in scores]}\")\n",
        "print(f\"Mean Accuracy: {sum(scores)/len(scores):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Survival Prediction Titanic"
      ],
      "metadata": {
        "id": "F0hW7opeDiWr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rBerI1QpDhga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTkvElNUDS9s"
      },
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "| Step | What it does |\n",
        "|------|--------------|\n",
        "| **1. Initialize Network** | Random weights for each neuron (including bias) |\n",
        "| **2. Forward Propagation** | Compute neuron outputs layer by layer using sigmoid |\n",
        "| **3. Backpropagation** | Compute error deltas from output → input using chain rule |\n",
        "| **4. Train Network** | SGD: repeat forward + backward pass for multiple epochs |\n",
        "| **5. Predict** | argmax of output layer activations |\n",
        "| **6. Case Study** | ~93–96% accuracy on Wheat Seeds dataset with 5-fold CV |\n",
        "\n",
        "### Key Formulas\n",
        "\n",
        "| Formula | Expression |\n",
        "|---------|------------|\n",
        "| Activation | $\\displaystyle a = \\sum_i w_i x_i + b$ |\n",
        "| Sigmoid | $\\displaystyle \\sigma(x) = \\frac{1}{1+e^{-x}}$ |\n",
        "| Sigmoid derivative | $\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$ |\n",
        "| Output delta | $\\delta^{\\text{out}} = (\\hat{y} - y) \\cdot \\sigma'(\\text{out})$ |\n",
        "| Hidden delta | $\\delta^{\\text{hid}} = \\left(\\sum_k w_k \\delta_k^{\\text{out}}\\right) \\cdot \\sigma'(\\text{out})$ |\n",
        "| Weight update | $w_i \\leftarrow w_i - \\eta \\cdot \\delta \\cdot x_i$ |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}