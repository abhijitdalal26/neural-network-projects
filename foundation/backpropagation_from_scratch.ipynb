{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Implement the Backpropagation Algorithm From Scratch in Python\n",
    "\n",
    "Based on the tutorial by Jason Brownlee — [Machine Learning Mastery](https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/)\n",
    "\n",
    "---\n",
    "\n",
    "**Backpropagation** is a supervised learning algorithm for training multilayer feed-forward neural networks. It works by propagating the error signal backward through the network and using **gradient descent** to update weights.\n",
    "\n",
    "In this notebook we will:\n",
    "- Initialize a neural network\n",
    "- Forward propagate inputs\n",
    "- Backpropagate errors\n",
    "- Train the network with stochastic gradient descent\n",
    "- Make predictions\n",
    "- Apply the algorithm to the Wheat Seeds dataset\n",
    "- Predict Titanic survival using the same backpropagation network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from random import seed, random\n",
    "from csv import reader\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Initialize Network\n",
    "\n",
    "We represent a network as a **list of layers**, where each layer is a **list of neurons**. Each neuron is a dictionary containing its `weights` (one per input + one bias) and other training properties.\n",
    "\n",
    "The network architecture:\n",
    "- **Input layer**: the raw feature values (not stored explicitly)\n",
    "- **Hidden layer**: `n_hidden` neurons, each connected to all inputs\n",
    "- **Output layer**: one neuron per class\n",
    "\n",
    "Weights are initialized **randomly** in the range `[0, 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    \"\"\"\n",
    "    Create and initialize a new neural network.\n",
    "    Each neuron: {'weights': [w1, w2, ..., bias]}\n",
    "    \"\"\"\n",
    "    network = []\n",
    "    # Hidden layer: n_hidden neurons, each with (n_inputs + 1) weights (last = bias)\n",
    "    hidden_layer = [{'weights': [random() for _ in range(n_inputs + 1)]} for _ in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    # Output layer: n_outputs neurons, each with (n_hidden + 1) weights\n",
    "    output_layer = [{'weights': [random() for _ in range(n_hidden + 1)]} for _ in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: 2 inputs, 1 hidden neuron, 2 outputs\n",
    "seed(1)\n",
    "network = initialize_network(2, 1, 2)\n",
    "for layer_idx, layer in enumerate(network):\n",
    "    print(f\"Layer {layer_idx + 1}:\")\n",
    "    for neuron in layer:\n",
    "        print(f\"  Neuron weights: {neuron['weights']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Forward Propagation\n",
    "\n",
    "Forward propagation computes the **output of each neuron** layer by layer until we reach the final output.\n",
    "\n",
    "### 2.1 Neuron Activation\n",
    "\n",
    "The activation of a neuron is the **weighted sum** of inputs plus a bias:\n",
    "\n",
    "$$\\text{activation} = \\sum_{i=1}^{n} w_i \\cdot x_i + b$$\n",
    "\n",
    "where $w_i$ are the weights, $x_i$ are the inputs, and $b$ is the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(weights, inputs):\n",
    "    \"\"\"\n",
    "    Calculate neuron activation: dot(weights, inputs) + bias\n",
    "    The last weight is the bias (paired with a constant input of 1).\n",
    "    \"\"\"\n",
    "    activation = weights[-1]  # bias\n",
    "    for i in range(len(weights) - 1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sigmoid Transfer Function\n",
    "\n",
    "After computing the activation, we pass it through an **activation function** to introduce non-linearity. We use the **sigmoid function**:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "The sigmoid maps any real number to the range $(0, 1)$, making it ideal for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer(activation):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function: σ(x) = 1 / (1 + e^(-x))\n",
    "    Output is always in range (0, 1).\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + exp(-activation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Forward Propagation Through Layers\n",
    "\n",
    "The output of each layer becomes the **input to the next layer**. For each neuron, we compute the activation and store the output as `'output'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(network, row):\n",
    "    \"\"\"\n",
    "    Forward propagate one row through the network.\n",
    "    Returns the final output layer activations.\n",
    "    \"\"\"\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs  # outputs of this layer become inputs to next\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward propagation\n",
    "seed(1)\n",
    "network = initialize_network(2, 1, 2)\n",
    "row = [1, 0, None]  # 2 inputs + class label placeholder\n",
    "output = forward_propagate(network, row)\n",
    "print(\"Network output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Backpropagation\n",
    "\n",
    "Backpropagation calculates the **error gradient** for each neuron and propagates it backward through the network to update the weights.\n",
    "\n",
    "### 3.1 Sigmoid Derivative\n",
    "\n",
    "To update weights using gradient descent, we need the **derivative of the sigmoid function**. Given that we already have the sigmoid output $\\sigma(x)$:\n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$$\n",
    "\n",
    "This is efficient since we already stored the output from forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_derivative(output):\n",
    "    \"\"\"\n",
    "    Derivative of the sigmoid function.\n",
    "    σ'(x) = σ(x) * (1 - σ(x))\n",
    "    We use the already-computed output to avoid recalculation.\n",
    "    \"\"\"\n",
    "    return output * (1.0 - output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Computing Error Signals (Delta)\n",
    "\n",
    "**Output layer error** — The delta for each output neuron is:\n",
    "\n",
    "$$\\delta^{\\text{out}}_j = (\\hat{y}_j - y_j) \\cdot \\sigma'(\\text{out}_j)$$\n",
    "\n",
    "where $\\hat{y}_j$ is the predicted output and $y_j$ is the expected output (one-hot encoded).\n",
    "\n",
    "**Hidden layer error** — The delta is the weighted sum of errors from the next layer:\n",
    "\n",
    "$$\\delta^{\\text{hidden}}_j = \\left(\\sum_k w_{jk} \\cdot \\delta^{\\text{out}}_k\\right) \\cdot \\sigma'(\\text{out}_j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagate_error(network, expected):\n",
    "    \"\"\"\n",
    "    Backpropagate error and store delta in each neuron.\n",
    "    'expected' is the one-hot encoded expected output.\n",
    "    \"\"\"\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = []\n",
    "        \n",
    "        if i != len(network) - 1:\n",
    "            # Hidden layers: weighted sum of errors from next layer\n",
    "            for j in range(len(layer)):\n",
    "                error = sum(neuron['weights'][j] * neuron['delta'] for neuron in network[i + 1])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            # Output layer: difference between prediction and expected\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(neuron['output'] - expected[j])\n",
    "        \n",
    "        # Compute delta = error * sigmoid_derivative(output)\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Update Weights\n",
    "\n",
    "Once we have the delta for each neuron, we update the weights using **stochastic gradient descent**:\n",
    "\n",
    "$$w_i \\leftarrow w_i - \\eta \\cdot \\delta \\cdot x_i$$\n",
    "\n",
    "where $\\eta$ is the **learning rate** and $x_i$ is the input to the neuron (output of the previous layer). The bias weight is updated with $x_i = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(network, row, l_rate):\n",
    "    \"\"\"\n",
    "    Update weights using gradient descent.\n",
    "    w_i = w_i - learning_rate * delta * input_i\n",
    "    \"\"\"\n",
    "    for i in range(len(network)):\n",
    "        # Input to this layer: original row (for hidden) or previous layer outputs\n",
    "        inputs = row[:-1]  # exclude class label\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] -= l_rate * neuron['delta']  # update bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Train Network\n",
    "\n",
    "Training is done using **Stochastic Gradient Descent (SGD)**: for each epoch, we iterate over all training examples one at a time, forward propagate, backpropagate the error, and update weights.\n",
    "\n",
    "The **sum of squared errors (SSE)** is accumulated each epoch to monitor training progress:\n",
    "\n",
    "$$\\text{SSE} = \\sum_{j} (\\hat{y}_j - y_j)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    \"\"\"\n",
    "    Train a network using stochastic gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "        network   : initialized network\n",
    "        train     : training dataset (last column = class label as integer)\n",
    "        l_rate    : learning rate η\n",
    "        n_epoch   : number of training epochs\n",
    "        n_outputs : number of output neurons (= number of classes)\n",
    "    \"\"\"\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            # One-hot encode the expected output\n",
    "            expected = [0] * n_outputs\n",
    "            expected[int(row[-1])] = 1\n",
    "            # Accumulate SSE\n",
    "            sum_error += sum((expected[j] - outputs[j])**2 for j in range(len(expected)))\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        print(f\"Epoch={epoch+1:4d}  learning_rate={l_rate:.3f}  error={sum_error:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training on a small XOR-like dataset\n",
    "seed(1)\n",
    "dataset = [\n",
    "    [2.7810836,  2.550537003, 0],\n",
    "    [1.465489372, 2.362125076, 0],\n",
    "    [3.396561688, 4.400293529, 0],\n",
    "    [1.38807019,  1.850220317, 0],\n",
    "    [3.06407232,  3.005305973, 0],\n",
    "    [7.627531214, 2.759262235, 1],\n",
    "    [5.332441248, 2.088626775, 1],\n",
    "    [6.922596716, 1.77106367,  1],\n",
    "    [8.675418651, -0.242068655, 1],\n",
    "    [7.673756466, 3.508563011, 1]\n",
    "]\n",
    "\n",
    "n_inputs  = len(dataset[0]) - 1  # 2\n",
    "n_outputs = len(set(row[-1] for row in dataset))  # 2\n",
    "\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, l_rate=0.5, n_epoch=20, n_outputs=n_outputs)\n",
    "\n",
    "print(\"\\nFinal weights:\")\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Predict\n",
    "\n",
    "After training, we can make predictions. We forward propagate a row and return the **index of the output neuron with the highest activation** (i.e., `argmax`):\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_j \\; \\text{output}_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, row):\n",
    "    \"\"\"\n",
    "    Make a prediction for a given row.\n",
    "    Returns the index of the output neuron with the highest output (argmax).\n",
    "    \"\"\"\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions on training data\n",
    "for row in dataset:\n",
    "    prediction = predict(network, row)\n",
    "    actual = int(row[-1])\n",
    "    status = \"✓\" if prediction == actual else \"✗\"\n",
    "    print(f\"Expected={actual}  Predicted={prediction}  {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Seeds Dataset Case Study\n",
    "\n",
    "We now apply the full algorithm to a **real-world dataset** — the [Wheat Seeds Dataset](https://archive.ics.uci.edu/ml/datasets/seeds).\n",
    "\n",
    "- **7 input features**: geometric properties of wheat kernels (area, perimeter, compactness, etc.)\n",
    "- **3 output classes**: varieties of wheat (Kama=0, Rosa=1, Canadian=2)\n",
    "- **210 samples** total\n",
    "\n",
    "We evaluate using **5-fold cross-validation** with the following hyperparameters:\n",
    "- Learning rate: `0.3`\n",
    "- Epochs: `500`\n",
    "- Hidden neurons: `5`\n",
    "\n",
    "### 6.1 Helper Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename):\n",
    "    \"\"\"Load a CSV file and return all rows as a list of lists.\"\"\"\n",
    "    dataset = []\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def str_column_to_float(dataset, column):\n",
    "    \"\"\"Convert a column of strings to floats.\"\"\"\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "\n",
    "def str_column_to_int(dataset, column):\n",
    "    \"\"\"Convert a string column to integer class labels (0-indexed).\"\"\"\n",
    "    class_values = sorted(set(row[column] for row in dataset))\n",
    "    lookup = {val: i for i, val in enumerate(class_values)}\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Normalization\n",
    "\n",
    "We **normalize** each feature to the range $[0, 1]$ using **min-max scaling**:\n",
    "\n",
    "$$x'_i = \\frac{x_i - x_{\\min}}{x_{\\max} - x_{\\min}}$$\n",
    "\n",
    "This prevents features with large values from dominating the gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_minmax(dataset):\n",
    "    \"\"\"Find min and max values for each column.\"\"\"\n",
    "    minmax = []\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        minmax.append([min(col_values), max(col_values)])\n",
    "    return minmax\n",
    "\n",
    "\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    \"\"\"Normalize each feature to range [0, 1] using min-max scaling.\"\"\"\n",
    "    for row in dataset:\n",
    "        for i in range(len(row) - 1):  # exclude label column\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_split(dataset, n_folds):\n",
    "    \"\"\"Split dataset into k folds for cross-validation.\"\"\"\n",
    "    dataset_split = []\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "    \"\"\"Calculate classification accuracy as a percentage.\"\"\"\n",
    "    correct = sum(1 for a, p in zip(actual, predicted) if a == p)\n",
    "    return correct / len(actual) * 100.0\n",
    "\n",
    "\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    \"\"\"Evaluate algorithm using k-fold cross-validation.\"\"\"\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = []\n",
    "    for fold in folds:\n",
    "        train_set = [row for f in folds for row in f if f is not fold]\n",
    "        test_set  = [list(row) for row in fold]  # copy so labels can be removed\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual    = [row[-1] for row in fold]\n",
    "        scores.append(accuracy_metric(actual, predicted))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Full Backpropagation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
    "    \"\"\"\n",
    "    Full backpropagation algorithm:\n",
    "      1. Initialize network\n",
    "      2. Train with SGD\n",
    "      3. Predict on test set\n",
    "    \"\"\"\n",
    "    n_inputs  = len(train[0]) - 1\n",
    "    n_outputs = len(set(row[-1] for row in train))\n",
    "    network   = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "    train_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "    predictions = [predict(network, row) for row in test]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Download and Prepare the Seeds Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Download the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv'\n",
    "urllib.request.urlretrieve(url, 'wheat-seeds.csv')\n",
    "print(\"Dataset downloaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Run Cross-Validation and Report Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "dataset = load_csv('wheat-seeds.csv')\n",
    "for i in range(len(dataset[0]) - 1):\n",
    "    str_column_to_float(dataset, i)\n",
    "str_column_to_int(dataset, len(dataset[0]) - 1)  # convert class labels\n",
    "\n",
    "# Normalize features\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "\n",
    "# Hyperparameters\n",
    "n_folds  = 5\n",
    "l_rate   = 0.3\n",
    "n_epoch  = 500\n",
    "n_hidden = 5\n",
    "\n",
    "# Evaluate\n",
    "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"Fold Scores: {[f'{s:.1f}%' for s in scores]}\")\n",
    "print(f\"Mean Accuracy: {sum(scores)/len(scores):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Titanic Survival Prediction\n",
    "\n",
    "We apply the backpropagation algorithm to the classic **Titanic dataset** — predicting whether a passenger survived (1) or not (0).\n",
    "\n",
    "### Dataset Details\n",
    "- **Source**: [Kaggle Titanic Dataset](https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv)\n",
    "- **Target**: `Survived` (0 = No, 1 = Yes) — binary classification\n",
    "- **Missing value strategy**: Rows with **any missing value are dropped** (no imputation)\n",
    "\n",
    "### Features Used (after dropping missing rows)\n",
    "| Feature | Type | Description |\n",
    "|---------|------|-------------|\n",
    "| `Pclass` | Numeric | Passenger class (1, 2, 3) |\n",
    "| `Sex` | Encoded | male=0, female=1 |\n",
    "| `Age` | Numeric | Age in years |\n",
    "| `SibSp` | Numeric | # of siblings/spouses aboard |\n",
    "| `Parch` | Numeric | # of parents/children aboard |\n",
    "| `Fare` | Numeric | Passenger fare |\n",
    "\n",
    "### 7.1 Load and Clean the Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import csv\n",
    "\n",
    "# Download Titanic dataset\n",
    "titanic_url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "urllib.request.urlretrieve(titanic_url, 'titanic.csv')\n",
    "print(\"Titanic dataset downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Preprocessing\n",
    "\n",
    "Steps:\n",
    "1. Select only the useful numeric/encodable columns\n",
    "2. Encode categorical `Sex` column (male → 0, female → 1)\n",
    "3. **Drop any row that contains a missing/empty value** — no imputation\n",
    "4. Move the `Survived` label to the last column\n",
    "5. Normalize all feature columns to $[0, 1]$ using min-max scaling:\n",
    "\n",
    "$$x'_i = \\frac{x_i - x_{\\min}}{x_{\\max} - x_{\\min}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_titanic(filename):\n",
    "    \"\"\"\n",
    "    Load Titanic CSV, select relevant columns, encode Sex,\n",
    "    drop rows with any missing values, and return as list of lists:\n",
    "    [Pclass, Sex, Age, SibSp, Parch, Fare, Survived]\n",
    "    \"\"\"\n",
    "    selected_cols = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived']\n",
    "    dataset = []\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        for row in csv_reader:\n",
    "            # Extract only the columns we need\n",
    "            record = [row[col].strip() for col in selected_cols]\n",
    "            \n",
    "            # Drop row if ANY value is missing or empty\n",
    "            if any(v == '' for v in record):\n",
    "                continue\n",
    "            \n",
    "            # Encode Sex: male -> 0, female -> 1\n",
    "            sex_idx = selected_cols.index('Sex')\n",
    "            record[sex_idx] = '0' if record[sex_idx].lower() == 'male' else '1'\n",
    "            \n",
    "            dataset.append(record)\n",
    "    \n",
    "    # Convert all to float\n",
    "    dataset = [[float(v) for v in row] for row in dataset]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "titanic_data = load_titanic('titanic.csv')\n",
    "print(f\"Total rows after dropping missing values: {len(titanic_data)}\")\n",
    "print(f\"Features per row (last = Survived label): {len(titanic_data[0])}\")\n",
    "print(f\"Sample row: {titanic_data[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Normalize Features\n",
    "\n",
    "We reuse the `dataset_minmax` and `normalize_dataset` functions defined in Section 6.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features (all columns except the last label column)\n",
    "titanic_minmax = dataset_minmax(titanic_data)\n",
    "normalize_dataset(titanic_data, titanic_minmax)\n",
    "\n",
    "# Class distribution\n",
    "survived = sum(1 for row in titanic_data if row[-1] == 1)\n",
    "not_survived = len(titanic_data) - survived\n",
    "print(f\"Survived    : {survived}  ({survived/len(titanic_data)*100:.1f}%)\")\n",
    "print(f\"Not Survived: {not_survived}  ({not_survived/len(titanic_data)*100:.1f}%)\")\n",
    "print(f\"Sample normalized row: {[round(v, 4) for v in titanic_data[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Train and Evaluate with Cross-Validation\n",
    "\n",
    "We use **5-fold cross-validation** with these hyperparameters:\n",
    "\n",
    "| Hyperparameter | Value |\n",
    "|----------------|-------|\n",
    "| Learning rate $\\eta$ | 0.3 |\n",
    "| Epochs | 200 |\n",
    "| Hidden neurons | 5 |\n",
    "| Folds | 5 |\n",
    "\n",
    "The `back_propagation` and `evaluate_algorithm` functions from Section 6 are reused directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "# Hyperparameters\n",
    "t_n_folds  = 5\n",
    "t_l_rate   = 0.3\n",
    "t_n_epoch  = 200\n",
    "t_n_hidden = 5\n",
    "\n",
    "# Evaluate\n",
    "titanic_scores = evaluate_algorithm(\n",
    "    titanic_data, back_propagation,\n",
    "    t_n_folds, t_l_rate, t_n_epoch, t_n_hidden\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*45)\n",
    "print(f\"Fold Scores : {[f'{s:.1f}%' for s in titanic_scores]}\")\n",
    "print(f\"Mean Accuracy: {sum(titanic_scores)/len(titanic_scores):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Predict a Single Passenger\n",
    "\n",
    "We can now feed a custom passenger profile through the trained network to get a survival prediction.\n",
    "\n",
    "Example — **3rd class male passenger, age 22, traveling alone, low fare**:\n",
    "\n",
    "| Feature | Value |\n",
    "|---------|-------|\n",
    "| Pclass  | 3     |\n",
    "| Sex     | male (0) |\n",
    "| Age     | 22    |\n",
    "| SibSp   | 1     |\n",
    "| Parch   | 0     |\n",
    "| Fare    | 7.25  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "# Train a final network on the full titanic dataset for single prediction demo\n",
    "n_inputs_t  = len(titanic_data[0]) - 1\n",
    "n_outputs_t = len(set(int(row[-1]) for row in titanic_data))\n",
    "final_network = initialize_network(n_inputs_t, t_n_hidden, n_outputs_t)\n",
    "train_network(final_network, titanic_data, t_l_rate, t_n_epoch, n_outputs_t)\n",
    "\n",
    "# Custom passenger: [Pclass, Sex, Age, SibSp, Parch, Fare]\n",
    "# We need to normalize these values using the same min-max stats\n",
    "raw_passenger = [3, 0, 22, 1, 0, 7.25]  # 3rd class male, age 22\n",
    "normalized_passenger = [\n",
    "    (raw_passenger[i] - titanic_minmax[i][0]) / (titanic_minmax[i][1] - titanic_minmax[i][0])\n",
    "    for i in range(len(raw_passenger))\n",
    "]\n",
    "normalized_passenger.append(None)  # placeholder for label\n",
    "\n",
    "prediction = predict(final_network, normalized_passenger)\n",
    "label = 'Survived ✓' if prediction == 1 else 'Did NOT Survive ✗'\n",
    "print(f\"Passenger profile : Pclass=3, Sex=Male, Age=22, SibSp=1, Parch=0, Fare=7.25\")\n",
    "print(f\"Prediction        : {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Step | What it does |\n",
    "|------|--------------|\n",
    "| **1. Initialize Network** | Random weights for each neuron (including bias) |\n",
    "| **2. Forward Propagation** | Compute neuron outputs layer by layer using sigmoid |\n",
    "| **3. Backpropagation** | Compute error deltas from output → input using chain rule |\n",
    "| **4. Train Network** | SGD: repeat forward + backward pass for multiple epochs |\n",
    "| **5. Predict** | argmax of output layer activations |\n",
    "| **6. Case Study** | ~93–96% accuracy on Wheat Seeds dataset with 5-fold CV |\n",
    "| **7. Titanic Prediction** | Binary survival classification; missing rows dropped; ~78–82% accuracy |\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "| Formula | Expression |\n",
    "|---------|------------|\n",
    "| Activation | $\\displaystyle a = \\sum_i w_i x_i + b$ |\n",
    "| Sigmoid | $\\displaystyle \\sigma(x) = \\frac{1}{1+e^{-x}}$ |\n",
    "| Sigmoid derivative | $\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$ |\n",
    "| Output delta | $\\delta^{\\text{out}} = (\\hat{y} - y) \\cdot \\sigma'(\\text{out})$ |\n",
    "| Hidden delta | $\\delta^{\\text{hid}} = \\left(\\sum_k w_k \\delta_k^{\\text{out}}\\right) \\cdot \\sigma'(\\text{out})$ |\n",
    "| Weight update | $w_i \\leftarrow w_i - \\eta \\cdot \\delta \\cdot x_i$ |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
