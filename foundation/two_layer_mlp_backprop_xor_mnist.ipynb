{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "762fd5d6",
   "metadata": {},
   "source": [
    "# From Perceptron to Deep Learning\n",
    "## 2-Layer MLP from Scratch (NumPy Only)\n",
    "\n",
    "Goals:\n",
    "- Implement forward pass manually\n",
    "- Implement full backpropagation\n",
    "- Solve XOR problem\n",
    "- Observe vanishing gradients (Sigmoid vs ReLU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c89bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da7678c",
   "metadata": {},
   "source": [
    "\n",
    "## Model Architecture\n",
    "\n",
    "We build a 2-layer MLP:\n",
    "\n",
    "Input → Hidden Layer → Output\n",
    "\n",
    "Hidden layer:\n",
    "\\[\n",
    "z_1 = XW_1 + b_1\n",
    "\\]\n",
    "\\[\n",
    "a_1 = \\sigma(z_1)\n",
    "\\]\n",
    "\n",
    "Output layer:\n",
    "\\[\n",
    "z_2 = a_1 W_2 + b_2\n",
    "\\]\n",
    "\\[\n",
    "\\hat{y} = \\sigma(z_2)\n",
    "\\]\n",
    "\n",
    "Loss (Binary Cross Entropy):\n",
    "\\[\n",
    "L = -[y\\log(\\hat y) + (1-y)\\log(1-\\hat y)]\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74258048",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation=\"sigmoid\"):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        \n",
    "        if self.activation == \"sigmoid\":\n",
    "            self.a1 = sigmoid(self.z1)\n",
    "        else:\n",
    "            self.a1 = relu(self.z1)\n",
    "\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.y_hat = sigmoid(self.z2)\n",
    "        return self.y_hat\n",
    "\n",
    "    def backward(self, y, lr=0.1):\n",
    "        m = y.shape[0]\n",
    "        delta2 = (self.y_hat - y)\n",
    "\n",
    "        dW2 = self.a1.T @ delta2 / m\n",
    "        db2 = np.mean(delta2, axis=0, keepdims=True)\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            delta1 = (delta2 @ self.W2.T) * sigmoid_deriv(self.a1)\n",
    "        else:\n",
    "            delta1 = (delta2 @ self.W2.T) * relu_deriv(self.z1)\n",
    "\n",
    "        dW1 = self.X.T @ delta1 / m\n",
    "        db1 = np.mean(delta1, axis=0, keepdims=True)\n",
    "\n",
    "        self.W2 -= lr * dW2\n",
    "        self.b2 -= lr * db2\n",
    "        self.W1 -= lr * dW1\n",
    "        self.b1 -= lr * db1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1731f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss(y, y_hat):\n",
    "    eps = 1e-8\n",
    "    return -np.mean(y*np.log(y_hat+eps) + (1-y)*np.log(1-y_hat+eps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39217a",
   "metadata": {},
   "source": [
    "## XOR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4259916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "\n",
    "y = np.array([[0],[1],[1],[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d208e",
   "metadata": {},
   "source": [
    "## Train on XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e1801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MLP(2, 4, 1, activation=\"sigmoid\")\n",
    "losses = []\n",
    "\n",
    "for epoch in range(5000):\n",
    "    y_hat = model.forward(X)\n",
    "    model.backward(y, lr=0.5)\n",
    "    l = loss(y, y_hat)\n",
    "    losses.append(l)\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch, l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc4d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss (XOR)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c7031",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(np.round(model.forward(X), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5174dbd2",
   "metadata": {},
   "source": [
    "## Tiny MNIST Experiment (Vanishing Gradient Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e98a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "\n",
    "X_m = mnist.data[:2000] / 255.0\n",
    "y_m = (mnist.target[:2000].astype(int) == 0).astype(int).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad46f16",
   "metadata": {},
   "source": [
    "### Train with Sigmoid Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sigmoid_model = MLP(784, 64, 1, activation=\"sigmoid\")\n",
    "losses_sigmoid = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    y_hat = sigmoid_model.forward(X_m)\n",
    "    sigmoid_model.backward(y_m, lr=0.1)\n",
    "    losses_sigmoid.append(loss(y_m, y_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08981c13",
   "metadata": {},
   "source": [
    "### Train with ReLU Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24315758",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "relu_model = MLP(784, 64, 1, activation=\"relu\")\n",
    "losses_relu = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    y_hat = relu_model.forward(X_m)\n",
    "    relu_model.backward(y_m, lr=0.1)\n",
    "    losses_relu.append(loss(y_m, y_hat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92707ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(losses_sigmoid, label=\"Sigmoid\")\n",
    "plt.plot(losses_relu, label=\"ReLU\")\n",
    "plt.legend()\n",
    "plt.title(\"Vanishing Gradient Effect\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363efc6f",
   "metadata": {},
   "source": [
    "\n",
    "## Observation\n",
    "\n",
    "Sigmoid networks learn slowly because:\n",
    "\n",
    "\\[\n",
    "\\sigma'(x) \\le 0.25\n",
    "\\]\n",
    "\n",
    "Gradients shrink every layer:\n",
    "\n",
    "\\[\n",
    "0.25^L \\rightarrow 0\n",
    "\\]\n",
    "\n",
    "This is the **vanishing gradient problem**.\n",
    "\n",
    "ReLU avoids this because:\n",
    "\n",
    "\\[\n",
    "\\frac{d}{dx}ReLU(x)=1 \\text{ (for positive inputs)}\n",
    "\\]\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
